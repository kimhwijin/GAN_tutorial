{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_emotion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12EYAkypMr19qS97q08KpTyIRO-hAEGvj",
      "authorship_tag": "ABX9TyNWCgIxz49I9Fc0arZauf5I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimhwijin/TensorflowWithKeras/blob/master/RNN/RNN_emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd5fp89W79fB"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ldayNN28tVt"
      },
      "source": [
        "def download_and_read(url):\n",
        "  local_file = url.split('/')[-1]\n",
        "  local_file = local_file.replace(\"%20\", \" \")\n",
        "  p = tf.keras.utils.get_file(local_file, url, extract=True, cache_dir=\".\")\n",
        "  local_folder = os.path.join(\"drive/MyDrive/Datasets/UCI_ML_ES\", local_file.split('.')[0])\n",
        "  labeled_sentences = []\n",
        "  for labeled_filename in os.listdir(local_folder):\n",
        "    if labeled_filename.endswith(\"_labelled.txt\"):\n",
        "      with open(os.path.join(local_folder, labeled_filename), \"r\") as f:\n",
        "        for line in f:\n",
        "          sentence, label = line.strip().split(\"\\t\")\n",
        "          labeled_sentences.append((sentence, label))\n",
        "\n",
        "  return labeled_sentences\n",
        "labeled_sentences = download_and_read(\n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\"\n",
        ")\n",
        "sentences = [s for (s, l) in labeled_sentences]\n",
        "labels = [l for (s, l) in labeled_sentences]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7faFpFQj_YXc",
        "outputId": "badd8965-4829-4993-c4e6-89bcd91d4ee5"
      },
      "source": [
        "#문장의 각 단어를  token으로 변환하기\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_counts)\n",
        "print(\"vocabulary size : {:d}\".format(vocab_size))\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v:k for (k, v) in word2idx.items()}"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary size : 5271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3EeIg90AXJU",
        "outputId": "7f30ee61-c1c0-4729-f15d-9c5a5794bf86"
      },
      "source": [
        "#정적인 입력크기를 정하기 위해, 대부분 문장을 수용할 수 있을 만큼의 시퀀스 길이를 정한다.\n",
        "seq_lengths = np.array([len(s.split()) for s in sentences])\n",
        "print([(p, np.percentile(seq_lengths, p)) for p in [75, 80, 90, 95, 99, 100]])\n",
        "#75% : 16단어 , 80% 18단어, 99% 36단어, 최대길이 71단어."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udw1f-5cBTNh"
      },
      "source": [
        "#최대 단어길이 64로 설정\n",
        "max_seqlen = 64\n",
        "\n",
        "#데이터 셋 생성\n",
        "sentences_as_ints = tokenizer.texts_to_sequences(sentences)\n",
        "#남은 부분은 0으로 채워준다.\n",
        "sentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(sentences_as_ints, maxlen=max_seqlen)\n",
        "labels_as_ints = np.array(labels)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((sentences_as_ints, labels_as_ints))\n",
        "\n",
        "dataset = dataset.suffle(10000)\n",
        "test_size = len(sentences) // 3\n",
        "val_size = (len(sentences) - test_size) // 10\n",
        "test_dataset = dataset.take(test_size)\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)\n",
        "\n",
        "batch_size = 64\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}