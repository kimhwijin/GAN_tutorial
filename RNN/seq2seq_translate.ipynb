{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_translate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1f4g_ihbjfN7sTtTCT4gX8w13s3DK76xJ",
      "authorship_tag": "ABX9TyML2Xy6GjNBSOXzETcONoJp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimhwijin/TensorflowWithKeras/blob/master/RNN/seq2seq_translate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuToWArdQcY4"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import unicodedata\n",
        "import zipfile\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXjiazl8Q3fD"
      },
      "source": [
        "def preprocessing_sentence(sent):\n",
        "    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) if unicodedata.category(c) != \"Mn\"])\n",
        "    #문자열 앞에 r 이붙으면 그대로 반환 r'abcd\\n' = abcd\\n\n",
        "    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n",
        "    #알파벳 또는 ! ? 제외하고 공백으로 치환\n",
        "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "    #공백문자를 띄어쓰기 한칸으로 변경\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "    sent = sent.lower()\n",
        "    return sent\n",
        "\n",
        "def download_and_read(url, num_sent_pairs=30000):\n",
        "\n",
        "    local_file = url.split('/')[-1]\n",
        "    drive_path = \"drive/MyDrive/Datasets/anki-fra-eng\"\n",
        "    data_path = os.join.path(drive_file, local_file)\n",
        "    if not os.path.exists(data_path):\n",
        "        os.system('-wget -O {:s} {:s}'.format(local_file, url))\n",
        "        with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_path)\n",
        "    file_path = os.join.path(data_path, 'fra_txt')\n",
        "    en_sents, fr_sents, fr_sents_out = [], [], []\n",
        "\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for i , line in enumerate(fin):\n",
        "            en_sent, fr_sent = line.strip().split('\\t')\n",
        "            en_sent = [w for w in preprocessing_sentence(en_sent).split()]\n",
        "            fr_sent = preprocessing_sentence(fr_sent)\n",
        "            fr_sent_in = [w for w in (\"BOS\" + fr_sent).split()]\n",
        "            fr_sent_out = [w for w in (fr_sent + \"EOS\").split()]\n",
        "            en_sents.append(en_sent)\n",
        "            fr_sents_in.append(fr_sent_in)\n",
        "            fr_sents_out.append(fr_sent_out)\n",
        "            if i >= num_sent_pairs - 1:\n",
        "                break\n",
        "    return en_sents, fr_sents_in, fr_sents_out\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieSwi1kdTOW6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}