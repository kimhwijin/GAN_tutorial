{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_translate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1f4g_ihbjfN7sTtTCT4gX8w13s3DK76xJ",
      "authorship_tag": "ABX9TyPWT7KarqC/6eIjWFWJMEn/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimhwijin/TensorflowWithKeras/blob/master/RNN/seq2seq_translate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuToWArdQcY4"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import unicodedata\n",
        "import zipfile\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXjiazl8Q3fD"
      },
      "source": [
        "def preprocessing_sentence(sent):\n",
        "    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) if unicodedata.category(c) != \"Mn\"])\n",
        "    #문자열 앞에 r 이붙으면 그대로 반환 r'abcd\\n' = abcd\\n\n",
        "    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n",
        "    #알파벳 또는 ! ? 제외하고 공백으로 치환\n",
        "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "    #공백문자를 띄어쓰기 한칸으로 변경\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "    sent = sent.lower()\n",
        "    return sent\n",
        "\n",
        "def download_and_read(url, num_sent_pairs=30000):\n",
        "\n",
        "    local_file = url.split('/')[-1]\n",
        "    drive_path = \"drive/MyDrive/Datasets/anki-fra-eng\"\n",
        "    data_path = os.join.path(drive_file, local_file)\n",
        "    if not os.path.exists(data_path):\n",
        "        os.system('-wget -O {:s} {:s}'.format(local_file, url))\n",
        "        with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_path)\n",
        "    file_path = os.join.path(data_path, 'fra_txt')\n",
        "    en_sents, fr_sents, fr_sents_out = [], [], []\n",
        "\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for i , line in enumerate(fin):\n",
        "            en_sent, fr_sent = line.strip().split('\\t')\n",
        "            en_sent = [w for w in preprocessing_sentence(en_sent).split()]\n",
        "            fr_sent = preprocessing_sentence(fr_sent)\n",
        "            fr_sent_in = [w for w in (\"BOS\" + fr_sent).split()]\n",
        "            fr_sent_out = [w for w in (fr_sent + \"EOS\").split()]\n",
        "            en_sents.append(en_sent)\n",
        "            fr_sents_in.append(fr_sent_in)\n",
        "            fr_sents_out.append(fr_sent_out)\n",
        "            if i >= num_sent_pairs - 1:\n",
        "                break\n",
        "    return en_sents, fr_sents_in, fr_sents_out\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BN8aEd_a3j2"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, num_timestemps, embedding_dim, encoder_dim, **kwargs):\n",
        "        super(Encoder, self).__init__(**kwargs)\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=timestemps)\n",
        "        self.rnn = tf.keras.layers.GRU(encoder_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, x, state):\n",
        "        x = self.embedding(x)\n",
        "        x, state = self.rnn(x, initial_state=state)\n",
        "        return x, state\n",
        "    def init_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, encoder_dim))\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, num_timestemps, embedding_dim, decoder_dim, **kwargs):\n",
        "        super(Decoder, self).__init__(**kwargs)\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, decoder_dim, input_length=num_timestemps)\n",
        "        self.rnn = tf.keras.layers.GRU(decoder_dim, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "    def call(self, x, state):\n",
        "        x = self.embedding(x)\n",
        "        x, state = self.rnn(x, state)\n",
        "        x = self.dense(x)\n",
        "        return x, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgJgq7a_aAwU"
      },
      "source": [
        "#하이퍼 파라미터\n",
        "NUM_SENT_PAIRS = 30000\n",
        "EMBEDDING_DIM = 256\n",
        "ENCODER_DIM, DECODER_DIM = 1024, 1024\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "#문장 데이터\n",
        "sents_en, sents_fr_in, sents_fr_out = download_and_read('https://www.manythings.org/anki/fra-eng.zip', NUM_SENT_PAIRS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieSwi1kdTOW6"
      },
      "source": [
        "#토크나이저\n",
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_en.fit_on_texts(sents_en)\n",
        "data_en = tokenizer_en.texts_to_sequences(sents_en)\n",
        "#뒤쪽 빈 부분을 채워줌\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')\n",
        "\n",
        "#데이터 및 토크나이저 설정\n",
        "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_fr.fit_on_texts(sents_fr_in)\n",
        "tokenizer_fr.fit_on_texts(sents_fr_out)\n",
        "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\n",
        "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding='post')\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding='post')\n",
        "\n",
        "#단어 개수\n",
        "vocab_size_en = len(tokenizer_en.word_index)\n",
        "vocab_size_fr = len(tokenizer_fr.word_index)\n",
        "word2idx_en = tokenizer_en.word_index\n",
        "idx2word_en = {v: k for k , v in word2idx_en.items()}\n",
        "word2idx_fr = tokenizer_fr.word_index\n",
        "idx2word_fr = {v: k for k , v in word2idx_fr.items()}\n",
        "print(\"단어 사이즈 (en) : {:d}, (fr) : {:d}\".format(vocab_size_en, vocab_size_fr))\n",
        "maxlen_en = data_en.shape[1]\n",
        "maxlen_fr = data_fr_out.shape[1]\n",
        "print(\"기준 시퀀셜 길이 (en) : {:d}, (fr) : {:d}\".format(maxlen_en, maxlen_fr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtHPC2qEaKAC"
      },
      "source": [
        "#test, train dataset // 1 : 3 비율\n",
        "dataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out))\n",
        "dataset = dataset.shuffle(10000)\n",
        "test_size = NUM_SENT_PAIRS // 4\n",
        "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\n",
        "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}