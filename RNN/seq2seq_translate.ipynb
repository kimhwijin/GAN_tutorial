{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_translate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1f4g_ihbjfN7sTtTCT4gX8w13s3DK76xJ",
      "authorship_tag": "ABX9TyOXZ7L6aWm7ANQHn4mP8y+m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimhwijin/TensorflowWithKeras/blob/master/RNN/seq2seq_translate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuToWArdQcY4"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import unicodedata\n",
        "import zipfile\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXjiazl8Q3fD"
      },
      "source": [
        "def preprocessing_sentence(sent):\n",
        "    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) if unicodedata.category(c) != \"Mn\"])\n",
        "    #문자열 앞에 r 이붙으면 그대로 반환 r'abcd\\n' = abcd\\n\n",
        "    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n",
        "    #알파벳 또는 ! ? 제외하고 공백으로 치환\n",
        "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "    #공백문자를 띄어쓰기 한칸으로 변경\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "    sent = sent.lower()\n",
        "    return sent\n",
        "\n",
        "def download_and_read(url, num_sent_pairs=30000):\n",
        "\n",
        "    local_file = url.split('/')[-1]\n",
        "    drive_path = \"drive/MyDrive/Datasets/anki-eng-frg\"\n",
        "    data_path = os.path.join(drive_path, local_file)\n",
        "    if not os.path.isfile(data_path):\n",
        "        os.system('wget -O {:s} -P {:s} {:s}'.format(local_file, drive_path, url))\n",
        "        with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_path)\n",
        "    file_path = os.path.join(drive_path, 'fra.txt')\n",
        "    en_sents, fr_sents_in, fr_sents_out = [], [], []\n",
        "\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for i , line in enumerate(fin):\n",
        "            en_sent, fr_sent, _ = line.strip().split('\\t')\n",
        "            en_sent = [w for w in preprocessing_sentence(en_sent).split()]\n",
        "            fr_sent = preprocessing_sentence(fr_sent)\n",
        "            fr_sent_in = [w for w in (\"BOS\" + fr_sent).split()]\n",
        "            fr_sent_out = [w for w in (fr_sent + \"EOS\").split()]\n",
        "            en_sents.append(en_sent)\n",
        "            fr_sents_in.append(fr_sent_in)\n",
        "            fr_sents_out.append(fr_sent_out)\n",
        "            if i >= num_sent_pairs - 1:\n",
        "                break\n",
        "    return en_sents, fr_sents_in, fr_sents_out\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BN8aEd_a3j2"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_timestemps, encoder_dim, **kwargs):\n",
        "        super(Encoder, self).__init__(**kwargs)\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=num_timestemps)\n",
        "        self.rnn = tf.keras.layers.GRU(self.encoder_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, x, state):\n",
        "        x = self.embedding(x)\n",
        "        x, state = self.rnn(x, initial_state=state)\n",
        "        return x, state\n",
        "    def init_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.encoder_dim))\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_timestemps, decoder_dim, **kwargs):\n",
        "        super(Decoder, self).__init__(**kwargs)\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, decoder_dim, input_length=num_timestemps)\n",
        "        self.rnn = tf.keras.layers.GRU(decoder_dim, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "    def call(self, x, state):\n",
        "        x = self.embedding(x)\n",
        "        x, state = self.rnn(x, state)\n",
        "        x = self.dense(x)\n",
        "        return x, state"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ92T_eSsUIy"
      },
      "source": [
        "#패딩된 부분 마스킹후, 로스 계산\n",
        "def loss_fn(ytrue, ypred):\n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0 ))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    loss = scce(ytrue, ypred, sample_weight=mask)\n",
        "    return loss\n",
        "\n",
        "@tf.function\n",
        "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n",
        "    with tf.GradientTape() as tape:\n",
        "        decoder_state = encoder_state\n",
        "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
        "        loss = loss_fn(decoder_out, decoder_pred)\n",
        "    variables = (encoder.trainable_variables + decoder.trainable_variables)\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgJgq7a_aAwU"
      },
      "source": [
        "#하이퍼 파라미터\n",
        "NUM_SENT_PAIRS = 30000\n",
        "EMBEDDING_DIM = 256\n",
        "ENCODER_DIM, DECODER_DIM = 1024, 1024\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "#\n",
        "checkpoint_dir = 'drive/MyDrive/Colab Notebooks/models/RNN_GRU_seq2seq'\n",
        "#문장 데이터\n",
        "sents_en, sents_fr_in, sents_fr_out = download_and_read('https://www.manythings.org/anki/fra-eng.zip', NUM_SENT_PAIRS)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieSwi1kdTOW6",
        "outputId": "1af5df7f-e15d-4327-828b-70966eb97d1a"
      },
      "source": [
        "#토크나이저\n",
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_en.fit_on_texts(sents_en)\n",
        "data_en = tokenizer_en.texts_to_sequences(sents_en)\n",
        "#뒤쪽 빈 부분을 채워줌\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')\n",
        "\n",
        "#데이터 및 토크나이저 설정\n",
        "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_fr.fit_on_texts(sents_fr_in)\n",
        "tokenizer_fr.fit_on_texts(sents_fr_out)\n",
        "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\n",
        "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding='post')\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding='post')\n",
        "\n",
        "#단어 개수\n",
        "vocab_size_en = len(tokenizer_en.word_index)\n",
        "vocab_size_fr = len(tokenizer_fr.word_index)\n",
        "word2idx_en = tokenizer_en.word_index\n",
        "idx2word_en = {v: k for k , v in word2idx_en.items()}\n",
        "word2idx_fr = tokenizer_fr.word_index\n",
        "idx2word_fr = {v: k for k , v in word2idx_fr.items()}\n",
        "print(\"단어 사이즈 (en) : {:d}, (fr) : {:d}\".format(vocab_size_en, vocab_size_fr))\n",
        "maxlen_en = data_en.shape[1]\n",
        "maxlen_fr = data_fr_out.shape[1]\n",
        "print(\"기준 시퀀셜 길이 (en) : {:d}, (fr) : {:d}\".format(maxlen_en, maxlen_fr))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 사이즈 (en) : 4354, (fr) : 8740\n",
            "기준 시퀀셜 길이 (en) : 8, (fr) : 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtHPC2qEaKAC"
      },
      "source": [
        "#test, train dataset // 1 : 3 비율\n",
        "dataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out))\n",
        "dataset = dataset.shuffle(10000)\n",
        "test_size = NUM_SENT_PAIRS // 4\n",
        "test_dataset = dataset.take(test_size).batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_dataset = dataset.skip(test_size).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6W74W8khgm-"
      },
      "source": [
        "encoder = Encoder(vocab_size_en + 1, EMBEDDING_DIM, maxlen_en, ENCODER_DIM)\n",
        "decoder = Decoder(vocab_size_fr + 1, EMBEDDING_DIM, maxlen_fr, ENCODER_DIM)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PatFnXd-h9ZI",
        "outputId": "0df78a46-3686-4752-baa6-4c1534a041b7"
      },
      "source": [
        "#shape test\n",
        "for encoder_in, decoder_in, decoder_out in train_dataset:\n",
        "    encoder_state = encoder.init_state(BATCH_SIZE)\n",
        "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
        "    decoder_state = encoder_state\n",
        "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
        "    break\n",
        "\n",
        "print(\"Encoder 입력 : \", encoder_in.shape)\n",
        "print(\"ENcoder 출력 : \", encoder_out.shape, \"state : \", encoder_state.shape)\n",
        "print(\"Decoder 입력 : \", decoder_in.shape)\n",
        "print(\"Decoder 출력 : \", decoder_out.shape, \"state : \", decoder_state.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder 입력 :  (64, 8)\n",
            "ENcoder 출력 :  (64, 8, 1024) state :  (64, 1024)\n",
            "Decoder 입력 :  (64, 15)\n",
            "Decoder 출력 :  (64, 15) state :  (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "332JZlxbix5N"
      },
      "source": [
        "def predict(encoder, decoder, batch_size, sents_en, data_en, sents_fr_out, word2idx_fr, idx2word_fr):\n",
        "    random_id = np.random.choice(len(sents_en))\n",
        "    #입력 문장\n",
        "    print(\"입력 : \", \"\".join(sents_en[random_id]))\n",
        "    #출력 문장\n",
        "    print(\"출력 : \", \"\".join(sents_fr_out[random_id]))\n",
        "    #배치차원 생성\n",
        "    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n",
        "    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\n",
        "    \n",
        "    #1 배치 사이스 state\n",
        "    encoder_state = encoder.init_state(batch_size=1)\n",
        "    #인코더\n",
        "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
        "    decoder_state = encoder_state\n",
        "    #처음 BOS 에 해당하는 index로 시작\n",
        "    decoder_in = tf.expand_dims(tf.constant([word2idx_fr[\"BOS\"]]), axis=0)\n",
        "    pred_sent_fr = []\n",
        "    #sequence 진행\n",
        "    while True:\n",
        "        #예상 단어\n",
        "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
        "        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n",
        "        pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\n",
        "        #문장에 추가\n",
        "        pred_sent_fr.append(pred_word)\n",
        "        #마지막\n",
        "        if pred_word == \"EOS\":\n",
        "            break\n",
        "        decoder_in = decoder_pred\n",
        "    print(\"predicted: \", \"\".join(pred_sent_fr))\n",
        "\n",
        "#BiLingual Evaluation Understudy 점수 / 테스트 dataset 전반에 걸쳐 수행\n",
        "def evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr):\n",
        "    bleu_scores = []\n",
        "    smooth_fn = SmoothingFunction()\n",
        "    for encoder_in, decoder_in, decoder_out in test_dataset:\n",
        "        encoder_state = encoder.init_state(BATCH_SIZE)\n",
        "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
        "        decoder_state = encoder_state\n",
        "\n",
        "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
        "\n",
        "    decoder_out = decoder_out.numpy()\n",
        "    decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\n",
        "\n",
        "    for i in range(decoder_out.shape[0]):\n",
        "        ref_sent = [idx2word_fr[j] for j in decoder_out[j].tolist() if j > 0]\n",
        "        hyp_sent = [idx2word_fr[j] for j in decoder_pred_[j].tolist() if j > 0]\n",
        "        #EOS 제거\n",
        "        ref_sent = ref_sent[0:-1]\n",
        "        hyp_sent = hyp_sent[0:-1]\n",
        "        bleu_score = sentence_bleu([ref_sent], hyp_sent, smoothing_function=smooth_fn.method1)\n",
        "        bleu_scores.append(bleu_score)\n",
        "    \n",
        "    return np.mean(np.array(bleu_scores))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "75rUMWTqkdoj",
        "outputId": "976a071e-d622-4ff5-c26e-444e14c1c013"
      },
      "source": [
        "#Train\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "\n",
        "num_epochs = 250\n",
        "eval_scores = []\n",
        "for e in range(num_epochs):\n",
        "    encoder_state = encoder.init_state(BATCH_SIZE)\n",
        "    print(len(train_dataset))\n",
        "    for batch, data in enumerate(train_dataset):\n",
        "        encoder_in, decoder_in, decoder_out = data\n",
        "        loss = train_step(encoder_in, decoder_in, decoder_out, encoder_state)\n",
        "    print(\"\\nEpochs : {}, loss : {:.4f}\".format(e + 1, loss.numpy()))\n",
        "\n",
        "    if e % 10 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "    \n",
        "    predict(encoder, decoder, BATCH_SIZE, sents_en, data_en, sents_fr_out, word2idx_fr, idx2word_fr)\n",
        "    eval_score = evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr)\n",
        "    print(type(eval_score))\n",
        "    print(\"eval score : {:.3e}\".format(eval_score))\n",
        "\n",
        "checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "351\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['encoder/embedding/embeddings:0', 'encoder/gru/gru_cell/kernel:0', 'encoder/gru/gru_cell/recurrent_kernel:0', 'encoder/gru/gru_cell/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['encoder/embedding/embeddings:0', 'encoder/gru/gru_cell/kernel:0', 'encoder/gru/gru_cell/recurrent_kernel:0', 'encoder/gru/gru_cell/bias:0'] when minimizing the loss.\n",
            "\n",
            "Epochs : 1, loss : 0.3081\n",
            "입력 :  allofusstoodup.\n",
            "출력 :  nousnoussommestoutesmisesdebout.EOS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0ea9a55c2650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents_fr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx_fr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2word_fr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0meval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_bleu_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx_fr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2word_fr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-a1890bdefe5c>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(encoder, decoder, batch_size, sents_en, data_en, sents_fr_out, word2idx_fr, idx2word_fr)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#예상 단어\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdecoder_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mdecoder_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpred_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx2word_fr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-24bf95c7a774>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, state)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    723\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'constants'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m   def call(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m       last_output, outputs, runtime, states = self._defun_gru_call(\n\u001b[0;32m--> 458\u001b[0;31m           inputs, initial_state, training, mask, row_lengths)\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36m_defun_gru_call\u001b[0;34m(self, inputs, initial_state, training, mask, sequence_lengths)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m           last_output, outputs, new_h, runtime = standard_gru(\n\u001b[0;32m--> 532\u001b[0;31m               **normal_gru_kwargs)\n\u001b[0m\u001b[1;32m    533\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         last_output, outputs, new_h, runtime = gru_with_backend_selection(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstandard_gru\u001b[0;34m(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\u001b[0m\n\u001b[1;32m    619\u001b[0m       \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msequence_lengths\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m       zero_output_for_mask=zero_output_for_mask)\n\u001b[0m\u001b[1;32m    622\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlast_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_RUNTIME_CPU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   4376\u001b[0m     \u001b[0;31m# the value is discarded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m     output_time_zero, _ = step_function(\n\u001b[0;32m-> 4378\u001b[0;31m         input_time_zero, tuple(initial_states) + tuple(constants))\n\u001b[0m\u001b[1;32m   4379\u001b[0m     output_ta = tuple(\n\u001b[1;32m   4380\u001b[0m         tensor_array_ops.TensorArray(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0;31m# inputs projected by all gate matrices at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0mmatrix_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m     \u001b[0mmatrix_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   2006\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   3488\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 3490\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5695\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   5696\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5697\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   5698\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5699\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}