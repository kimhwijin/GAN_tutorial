{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DBM_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIHXZqHCQSi7FigxCIc6L3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimhwijin/TensorflowWithKeras/blob/master/SUPERVISTED/DBM_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc-pnYeau7Aa"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wae--Z_vGCQ"
      },
      "source": [
        "#Class that defines the behavior of the RBM\n",
        "class RBM(object):\n",
        "    \n",
        "    def __init__(self, input_size, output_size, lr=1.0, batchsize=100):\n",
        "        \"\"\"\n",
        "        m: Number of neurons in visible layer\n",
        "        n: number of neurons in hidden layer\n",
        "        \"\"\"\n",
        "        #Defining the hyperparameters\n",
        "        self._input_size = input_size #Size of Visible\n",
        "        self._output_size = output_size #Size of outp\n",
        "        self.learning_rate = lr #The step used in gradient descent\n",
        "        self.batchsize = batchsize #The size of how much data will be used for training per sub iteration\n",
        "        \n",
        "        #Initializing weights and biases as matrices full of zeroes\n",
        "        self.w = tf.zeros([input_size, output_size], np.float32) #Creates and initializes the weights with 0\n",
        "        self.hb = tf.zeros([output_size], np.float32) #Creates and initializes the hidden biases with 0\n",
        "        self.vb = tf.zeros([input_size], np.float32) #Creates and initializes the visible biases with 0\n",
        "\n",
        "\n",
        "    #Forward Pass\n",
        "    def prob_h_given_v(self, visible, w, hb):\n",
        "        #Sigmoid \n",
        "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
        "\n",
        "    #Backward Pass\n",
        "    def prob_v_given_h(self, hidden, w, vb):\n",
        "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
        "    \n",
        "    #Generate the sample probability\n",
        "    def sample_prob(self, probs):\n",
        "        return tf.nn.relu(tf.sign(probs - tf.random.uniform(tf.shape(probs))))\n",
        "\n",
        "    #Training method for the model\n",
        "    def train(self, X, epochs=10):\n",
        "               \n",
        "        loss = []\n",
        "        for epoch in range(epochs):\n",
        "            #For each step/batch\n",
        "            for start, end in zip(range(0, len(X), self.batchsize),range(self.batchsize,len(X), self.batchsize)):\n",
        "                batch = X[start:end]\n",
        "                    \n",
        "                #Initialize with sample probabilities\n",
        "                    \n",
        "                h0 = self.sample_prob(self.prob_h_given_v(batch, self.w, self.hb))\n",
        "                v1 = self.sample_prob(self.prob_v_given_h(h0, self.w, self.vb))\n",
        "                h1 = self.prob_h_given_v(v1, self.w, self.hb)\n",
        "                    \n",
        "                #Create the Gradients\n",
        "                positive_grad = tf.matmul(tf.transpose(batch), h0)\n",
        "                negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
        "                    \n",
        "                #Update learning rates \n",
        "                self.w = self.w + self.learning_rate *(positive_grad - negative_grad) / tf.dtypes.cast(tf.shape(batch)[0],tf.float32)\n",
        "                self.vb = self.vb +  self.learning_rate * tf.reduce_mean(batch - v1, 0)\n",
        "                self.hb = self.hb +  self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n",
        "                    \n",
        "            #Find the error rate\n",
        "            err = tf.reduce_mean(tf.square(batch - v1))\n",
        "            print ('Epoch: %d' % epoch,'reconstruction error: %f' % err)\n",
        "            loss.append(err)\n",
        "                    \n",
        "        return loss\n",
        "    \n",
        "    def rbm_reconstruct(self,X):\n",
        "        h = tf.nn.sigmoid(tf.matmul(X, self.w) + self.hb)\n",
        "        reconstruct = tf.nn.sigmoid(tf.matmul(h, tf.transpose(self.w)) + self.vb)\n",
        "        return reconstruct\n",
        "\n",
        "    def rbm_output(self, X):\n",
        "        out = tf.nn.sigmoid(tf.matmul(X, self.w) + self.hb)\n",
        "        return out"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRlkhyk7vqhk",
        "outputId": "500f14a5-a017-4321-9c9b-dfede839bd31"
      },
      "source": [
        "(train_data, _), (test_data, _) = tf.keras.datasets.mnist.load_data()\n",
        "train_data = train_data / 255.0\n",
        "train_data = train_data.astype(np.float32)\n",
        "train_data = np.reshape(train_data, (train_data.shape[0], 784))\n",
        "\n",
        "test_data = test_data / np.float32(255)\n",
        "test_data = np.reshape(test_data, (test_data.shape[0], 784))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH6EAxFTvJ9Q",
        "outputId": "4e66fd61-5252-43fa-e27b-8c64d4229182"
      },
      "source": [
        "RBM_hidden_sizes = [500, 200, 50]\n",
        "\n",
        "input_data = train_data\n",
        "\n",
        "rbm_list= []\n",
        "input_size = input_data.shape[1]\n",
        "\n",
        "\n",
        "#784 -> 500, 500 -> 200, 200 -> 50\n",
        "#3개 RBM 네트워크를 stack 한다.\n",
        "for i , size in enumerate(RBM_hidden_sizes):\n",
        "    print(\"RBM: {:d} -> {:d}\".format(input_size, size))\n",
        "    rbm_list.append(RBM(input_size, size))\n",
        "    input_size = size\n",
        "\n",
        "for rbm in rbm_list:\n",
        "    print('New RBM training...')\n",
        "    rbm.train(tf.cast(input_data, tf.float32))\n",
        "    input_data = rbm.rbm_output(input_data)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RBM: 784 -> 500\n",
            "RBM: 500 -> 200\n",
            "RBM: 200 -> 50\n",
            "New RBM training...\n",
            "Epoch: 0 reconstruction error: 0.058084\n",
            "Epoch: 1 reconstruction error: 0.053617\n",
            "Epoch: 2 reconstruction error: 0.049083\n",
            "Epoch: 3 reconstruction error: 0.046656\n",
            "Epoch: 4 reconstruction error: 0.046165\n",
            "Epoch: 5 reconstruction error: 0.044353\n",
            "Epoch: 6 reconstruction error: 0.043565\n",
            "Epoch: 7 reconstruction error: 0.043504\n",
            "Epoch: 8 reconstruction error: 0.042443\n",
            "Epoch: 9 reconstruction error: 0.042670\n",
            "New RBM training...\n",
            "Epoch: 0 reconstruction error: 0.029680\n",
            "Epoch: 1 reconstruction error: 0.026589\n",
            "Epoch: 2 reconstruction error: 0.025454\n",
            "Epoch: 3 reconstruction error: 0.024009\n",
            "Epoch: 4 reconstruction error: 0.021566\n",
            "Epoch: 5 reconstruction error: 0.022320\n",
            "Epoch: 6 reconstruction error: 0.021863\n",
            "Epoch: 7 reconstruction error: 0.022237\n",
            "Epoch: 8 reconstruction error: 0.021484\n",
            "Epoch: 9 reconstruction error: 0.022986\n",
            "New RBM training...\n",
            "Epoch: 0 reconstruction error: 0.053768\n",
            "Epoch: 1 reconstruction error: 0.046305\n",
            "Epoch: 2 reconstruction error: 0.049109\n",
            "Epoch: 3 reconstruction error: 0.046712\n",
            "Epoch: 4 reconstruction error: 0.043999\n",
            "Epoch: 5 reconstruction error: 0.047960\n",
            "Epoch: 6 reconstruction error: 0.044949\n",
            "Epoch: 7 reconstruction error: 0.045614\n",
            "Epoch: 8 reconstruction error: 0.045382\n",
            "Epoch: 9 reconstruction error: 0.047468\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}