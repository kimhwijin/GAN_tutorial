{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Age_cGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1IKT8pFC-tInsJZjmS3s9ni8KMpI9l5ez",
      "authorship_tag": "ABX9TyMh2VsW19NfGeZ3FHTD6jAE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimhwijin/TensorflowWithKeras/blob/master/GAN/Age_cGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgPQHOEAodRW"
      },
      "source": [
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras  import Input, Model, layers, optimizers, utils\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from keras_preprocessing import image\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vibFeKaioSnk",
        "outputId": "9a559113-095e-4df7-ce2a-1f73fc43cd43"
      },
      "source": [
        "def load_data(wiki_dir, dataset='wiki'):\n",
        "  meta = loadmat(os.path.join(wiki_dir, \"{}.mat\".format(dataset)))\n",
        "  full_path = meta[dataset][0, 0][\"full_path\"][0]\n",
        "  dob = meta[dataset][0, 0][\"dob\"][0]\n",
        "  photo_taken = meta[dataset][0, 0][\"photo_taken\"][0]\n",
        "  age = [calculate_age(photo_taken[i], dob[i]) for i in range(len(dob))]\n",
        "  images = []\n",
        "  age_list = []\n",
        "\n",
        "  for index, image_path in enumerate(full_path):\n",
        "    images.append(image_path[0])\n",
        "    age_list.append(age[index])\n",
        "\n",
        "  return images, age_list\n",
        "\n",
        "def caluclate_age(taken, dob):\n",
        "  birth = datetime.fromordinal(max(int(dob) - 366, 1))\n",
        "  if brith.month < 7:\n",
        "    return taken - brith.year\n",
        "  else:\n",
        "    return taken - birth.year - 1\n",
        "os.path.isfile(\"drive/MyDrive/wiki_crop/wiki.mat\")\n",
        "#images, age_list = load_data(\"drive/MyDrive/wiki_crop/\", dataset='wiki')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44MVXf2Xs_Oh"
      },
      "source": [
        "def build_encoder():\n",
        "  input_layer = Input(shape=(64,64,3))\n",
        "  enc = layers.Conv2D(filters=32, kernel_size=5, strides=2, padding='same')(input_layer)\n",
        "  enc = layers.LeakyReLU(alpha=0.2)(enc)\n",
        "\n",
        "  enc = layers.Conv2D(filters=64, kernel_size=5, strides=2, padding='same')(enc)\n",
        "  enc = layers.BatchNormalization()(enc)\n",
        "  enc = layers.LeakyReLU(alpha=0.2)(enc)\n",
        "\n",
        "  enc = Conv2D(filters=128, kernel_size=5, strides=2, padding='same')(enc)\n",
        "  enc = layers.BatchNormalization()(enc)\n",
        "  enc = layers.LeakyReLU(alpha=0.2)(enc)\n",
        "\n",
        "  enc = Conv2D(filters=256, kernel_size=5, strides=2, padding='same')(enc)\n",
        "  enc = layers.BatchNormalization()(enc)\n",
        "  enc = layers.LeakyReLU(alpha=0.2)(enc)\n",
        "\n",
        "  enc = layers.Flatten()(enc)\n",
        "\n",
        "  enc = layers.Dense(4096)(enc)\n",
        "  enc = layers.BatchNormalization()(enc)\n",
        "  enc = layers.LeakyReLU(alpha=0.2)(enc)\n",
        "\n",
        "  enc = layers.Dense(100)(enc)\n",
        "  \n",
        "  model = Model(inputs=[input_layer], outputs=[enc])\n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am7dEFerD6Ze"
      },
      "source": [
        "\n",
        "def build_generator():\n",
        "  latent_dims = 100\n",
        "  num_classes = 6\n",
        "\n",
        "  input_z_noise = Input(shape=(latent_dims,))\n",
        "  input_label = Input(shape=(num_classes,))\n",
        "\n",
        "  x = layers.concatenate([input_z_noise, input_label])\n",
        "\n",
        "  x = layers.Dense(2048, input_dim=latent_dims+num_classes)(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "\n",
        "  x = layers.Dense(256 * 8 * 8)(x)\n",
        "  x = layers.BatchNormalization(momentum=0.8)(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "\n",
        "  x = layers.Reshape(8, 8, 256)(x)\n",
        "\n",
        "  x = layers.UpSampling2D(size=(2, 2))(x)\n",
        "  x = layers.Conv2D(filters=128, kernel_size=5, padding='same')(x)\n",
        "  x = layers.BatchNormalization(momentum=0.8)(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "  x = layers.UpSampling2D(size=(2, 2))(x)\n",
        "  x = layers.Conv2D(filters=64, kernel_size=5, padding='same')(x)\n",
        "  x = layers.BatchNormalization(momentum=0.8)(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "  x = layers.UpSampling2D(size=(2, 2))(x)\n",
        "  x = layers.Conv2D(filters=3, kernel_size=5, padding='same')(x)\n",
        "  x = layers.Activation('tahn')(x)\n",
        "\n",
        "  model = Model(inputs=[input_z_noise, input_label], output=[x])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKP0R50hGE_h"
      },
      "source": [
        "def build_discriminator():\n",
        "  input_shape = (64, 64, 3)\n",
        "  label_shape = (6, )\n",
        "\n",
        "  image_input = layers.Input(shape=input_shape)\n",
        "  label_input = layers.Input(shape=label_shape)\n",
        "\n",
        "  x = layers.Conv2D(filters=64, kernel_size=3, strides=2, pading='same')(image_input)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "  label_input1 = layers.Lambda(expand_label_input)(label_input)\n",
        "  def expand_label_input(x):\n",
        "    x = K.expand_dims(x, axis=1)\n",
        "    x = K.expand_dims(x, axis=1)\n",
        "    x = K.tile(x, [1, 32, 32, 1])\n",
        "    return x\n",
        "  \n",
        "  x = layers.concatenate([x, label_input1], axis=3)\n",
        "\n",
        "  x = layers.Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "  x = layers.Conv2D(256, kernel_size=3, strides=2, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "  x = layers.Conv2D(512, kernel_size=3, strides=2, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "  x = layers.Flatten()(x)\n",
        "\n",
        "  x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs=[image_input, label_input], outputs=[x])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n_iVzR9gsmi"
      },
      "source": [
        "data_dir = \"drive/MyDrive/Datasets/\"\n",
        "wiki_dir = os.path.join(data_dir, \"wiki_crop\")\n",
        "epochs = 500\n",
        "batch_size = 128\n",
        "image_shape = (64, 64, 3)\n",
        "z_shape = 100\n",
        "TRAIN_GAN = True\n",
        "TRAIN_ENCODER = False\n",
        "TRAIN_GAN_WITH_FR = False\n",
        "fr_image_shape = (192, 192, 3)\n",
        "\n",
        "dis_optimizer = optimizers.Adam(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=10e-8)\n",
        "gen_optimizer = optimizers.Adam(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=10e-8)\n",
        "adversarial_optimizer = optimizers.Adam(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=10e-8)\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss=['binary_crossentropy'], optimizers=dis_optimizer)\n",
        "\n",
        "generator = build_generator()\n",
        "generator.compile(loss=['binary_crossentropy'], optimizers=gen_optimizer)\n",
        "\n",
        "discriminator.trainable = False\n",
        "input_z_noise = Input(shape=(z_shape,))\n",
        "input_label = Input(shape=(6,)\n",
        "reconstruct_images = generator([input_z_noise, input_label])\n",
        "valid = discriminator([reconstruct_images, input_label])\n",
        "adversarial_model = Model(inputs=[input_z_noise, input_label])\n",
        "adversarial_model.compile(loss=['binary_crossentropy'], optimizer=gen_optimizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "622w3e8dBamf"
      },
      "source": [
        "tensorboard = TensorBoard(log_dir='drive/MyDrive/Colab Notebooks/logs/{}'.format(time.time()))\n",
        "tensorboard.set_model(generator)\n",
        "tensorboard.set_model(discriminator)\n",
        "\n",
        "images, age_list = load_data(wiki_dir=wiki_dir, dataset=\"wiki\")\n",
        "age_category = age_to_category(age_list)\n",
        "def age_to_category(age_list):\n",
        "  age_list1 = []\n",
        "  for age in age_list:\n",
        "    if 0 < age <= 18:\n",
        "      age_category = 0\n",
        "    elif 18 < age <= 29:\n",
        "      age_category = 1\n",
        "    elif 29 < age <= 39:\n",
        "      age_category = 2\n",
        "    elif 39 < age <= 49:\n",
        "      age_category = 3\n",
        "    elif 49 < age <= 59:\n",
        "      age_category = 4\n",
        "    elif 60 < age:\n",
        "      age_category = 5\n",
        "    age_list1.append(age_category)\n",
        "\n",
        "  return age_list1\n",
        "\n",
        "final_age_category = np.reshape(np.array(age_category), [len(age_category), 1])\n",
        "classes = 6\n",
        "y = utils.to_categorical(final_age_category, num_classes=classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q96BtHsjDPct"
      },
      "source": [
        "def load_images(data_dir, image_paths, image_shape):\n",
        "  images = None\n",
        "  for i , image_path in enumerate(image_paths):\n",
        "    print()\n",
        "    try:\n",
        "      loaded_image = image.load_img(os.path.join(data_dir, image_path), target_size=image_shape)\n",
        "      loaded_image = image.img_to_array(loaded_image)\n",
        "      loaded_image = np.expand_dims(loaded_image, axis=0)\n",
        "\n",
        "      if images is None:\n",
        "        images = loaded_image\n",
        "      else:\n",
        "        images = np.concatenate([images, loaded_image], axis=0)\n",
        "    except Exception as e:\n",
        "      print(\"Error: \", i, e)\n",
        "  return images\n",
        "\n",
        "loaded_images = load_images(wiki_dir, images, (images_shape[0], images_shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZMNLtpQEWzE"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(\"Epoch : {}\".format(epoch))\n",
        "  gen_losses = []\n",
        "  dis_losses = []\n",
        "\n",
        "  number_of_batches = int(len(loaded_images) / batch_size)\n",
        "  print(\"Number of batches : \" ,number_of_batches)\n",
        "\n",
        "  for index in range(number_of_batches):\n",
        "    print(\"Batch: {}\".format(index + 1))\n",
        "    images_batch = loaded_image[index * batch_size :(index + 1) * batch_size]\n",
        "    images_batch = images_batch / 127.5 - 1.0 #-1 to 1\n",
        "    images_batch = images_batch.astype(np.float32)\n",
        "    y_batch = y[index * batch_size : (index + 1) * batch_size]\n",
        "\n",
        "    z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
        "    #이미지 생성\n",
        "    initial_reconstruct_images = generator.predict_on_batch([z_noise, y_batch])\n",
        "\n",
        "    #판별기 훈련\n",
        "    d_loss_real = discriminator.train_on_batch([images_batch, y_batch], real_labels)\n",
        "    d_loss_fake = discriminator.train_on_batch([initial_reconstruct_images, y_batch], fake_labels)\n",
        "\n",
        "    #GAN 훈련\n",
        "    z_noise2 = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
        "    ramdon_labels = np.random.randint(0, 6, batch_size).reshape(1, -1)\n",
        "    random_labels = utils.to_categorical(random_labels, 6)\n",
        "\n",
        "    g_loss = adversarial_model.train_on_batch([z_noise2, random_labels], [1] * batch_size)\n",
        "\n",
        "    d_loss = 0.5 * np.add(d_loss_read, d_loss_fake)\n",
        "    print(\"d_loss: \", d_loss)\n",
        "    print(\"g_loss: \", g_loss)\n",
        "\n",
        "    gen_losses.append(g_loss)\n",
        "    dis_losses.append(d_loss)\n",
        "\n",
        "  write_log(tensorboard, 'g_loss', np.mean(gen_losses), epoch)\n",
        "  write_log(tensorboard, 'd_loss', np.mean(dis_losses), epoch)\n",
        "\n",
        "  if epoch % 10 == 1:\n",
        "    for i , img in enumerate(initial_reconstruct_images):\n",
        "      save_rgb_img(img, path=\"drive/MyDrive/Colab Notebooks/result_data/wiki_gan/img_{}_{}.png\".format(epoch,i))\n",
        "  \n",
        "generator.save_weight(\"drive/MyDrive/Colab Notebooks/models/wiki_gan/weight/generator.h5\")\n",
        "discriminator.save_weight(\"drive/MyDrive/Colab Notebooks/models/wiki_gan/weight/discriminator.h5\")\n",
        "\n",
        "generator.save(\"drive/MyDrive/Colab Notebooks/models/wiki_gan/model/generator.h5\")\n",
        "discriminator.save(\"drive/MyDrive/Colab Notebooks/models/wiki_gan/model/discriminator.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whhQ58DoIxkr"
      },
      "source": [
        "def save_rgb_img(img, path):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(1,1,1)\n",
        "  ax.imshow(img)\n",
        "  ax.axis(\"off\")\n",
        "  ax.set_title(\"Image\")\n",
        "  plt.savefig(path)\n",
        "  plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOrmoAOnBzPp",
        "outputId": "b5fe239d-dfe0-43e3-841a-f583db6151bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}