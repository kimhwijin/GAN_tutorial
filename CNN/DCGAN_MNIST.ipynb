{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkrvn0RVKa6bjPhWCbV5Ku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimhwijin/TensorflowWithKeras/blob/master/CNN/DCGAN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwdYFEwDnpVQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, datasets, optimizers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spyx2AUSn1j4"
      },
      "source": [
        "class DCGAN():\n",
        "  def __init__(self, rows, cols, channels, z=10):\n",
        "    #input shape\n",
        "    self.img_rows = rows\n",
        "    self.img_cols = cols\n",
        "    self.channels = channels\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    self.latent_dim = z\n",
        "\n",
        "    optimizer = optimizers.Adam(0.0002, 0.5)\n",
        "    \n",
        "    #build and compile the discriminator\n",
        "    self.discriminator = self.build_discriminator()\n",
        "    self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    #build the generator\n",
        "    self.generator = self.build_generator()\n",
        "\n",
        "    #the generator takes noise as input and generates imgs\n",
        "    z = tf.keras.Input(shape=(self.latent_dim,))\n",
        "    img = self.generator(z)\n",
        "\n",
        "    # The discriminator takes generated images as input and determines validity\n",
        "    valid = self.discriminator(img)\n",
        "\n",
        "    #생성기 판별기 결합\n",
        "    self.combined = models.Model(z, valid)\n",
        "    self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "  def build_generator(self):\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
        "    model.add(layers.Reshape((7, 7, 128)))\n",
        "    model.add(layers.UpSampling2D())\n",
        "    model.add(layers.Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation(\"relu\"))\n",
        "    model.add(layers.UpSampling2D())\n",
        "    model.add(layers.Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation(\"relu\"))\n",
        "    model.add(layers.Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
        "    model.add(layers.Activation(\"tanh\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    noise = tf.keras.Input(shape=(self.latent_dim,))\n",
        "    img = model(noise)\n",
        "\n",
        "    return models.Model(noise, img)\n",
        "\n",
        "  def build_discriminator(self):\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "    model.add(layers.Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    img = tf.keras.Input(shape=self.img_shape)\n",
        "    validity = model(img)\n",
        "\n",
        "    return models.Model(img, validity)\n",
        "\n",
        "\n",
        "  def train(self, epochs, batch_size=256, save_interval=50):\n",
        "    #load dataset\n",
        "    (X_train, _), (_, _) = datasets.mnist.load_data()\n",
        "\n",
        "    #Rescale -1 to 1\n",
        "    X_train = X_train / 127.5 -1.\n",
        "    X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "    #Adversarial ground truths\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      #판별기 훈련\n",
        "\n",
        "      #이미지의 랜덤 절반 선택\n",
        "      idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "      imgs = X_train[idx]\n",
        "\n",
        "      #noise 생성\n",
        "      noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "      gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "      #판별기 훈련 (진짜이미지 1 , 가짜 이미지 0)\n",
        "      d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "      d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "      #생성기 훈련\n",
        "\n",
        "      #판별기의 값이 1이 되도록\n",
        "      g_loss = self.combined.train_on_batch(noise, valid)\n",
        "      \n",
        "      print('%d [D loss : %f, acc : %.2f%%] [G loss : %f]' %(epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "      \n",
        "      if epoch % save_interval == 0:\n",
        "        self.save_imgs(epoch)\n",
        "        \n",
        "  def save_imgs(self, epoch):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "    gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "    #Rescale image 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r,c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "      for j in range(c):\n",
        "        axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "        axs[i,j].axis('off')\n",
        "        cnt += 1\n",
        "    fig.savefig('images/dcgan_mnist_%d.png' % epoch)\n",
        "\n",
        "    plt.close()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg0TTeJFnC8A",
        "outputId": "4853df2c-a1d5-40fe-98ab-b711ce64f601"
      },
      "source": [
        "dcgan = DCGAN(28, 28, 1)\n",
        "dcgan.train(epochs=500, batch_size=256, save_interval=50)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_46 (Conv2D)           (None, 14, 14, 32)        320       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_28 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_7 (ZeroPaddin (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_29 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 4097      \n",
            "=================================================================\n",
            "Total params: 393,729\n",
            "Trainable params: 392,833\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_14 (Dense)             (None, 6272)              68992     \n",
            "_________________________________________________________________\n",
            "reshape_6 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_12 (UpSampling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_13 (UpSampling (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 28, 28, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 28, 28, 1)         577       \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 291,713\n",
            "Trainable params: 291,329\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "0 [D loss : 1.214791, acc : 29.88%] [G loss : 1.106729]\n",
            "1 [D loss : 0.786670, acc : 54.88%] [G loss : 0.603568]\n",
            "2 [D loss : 0.753005, acc : 58.59%] [G loss : 0.405811]\n",
            "3 [D loss : 0.650020, acc : 66.02%] [G loss : 0.260821]\n",
            "4 [D loss : 0.499211, acc : 77.54%] [G loss : 0.129990]\n",
            "5 [D loss : 0.393816, acc : 83.79%] [G loss : 0.058545]\n",
            "6 [D loss : 0.334429, acc : 88.28%] [G loss : 0.030326]\n",
            "7 [D loss : 0.252929, acc : 92.97%] [G loss : 0.014574]\n",
            "8 [D loss : 0.196913, acc : 95.70%] [G loss : 0.011037]\n",
            "9 [D loss : 0.295853, acc : 89.06%] [G loss : 0.006039]\n",
            "10 [D loss : 0.284190, acc : 88.87%] [G loss : 0.005784]\n",
            "11 [D loss : 0.410438, acc : 83.59%] [G loss : 0.009257]\n",
            "12 [D loss : 0.568062, acc : 73.83%] [G loss : 0.017688]\n",
            "13 [D loss : 0.970600, acc : 53.52%] [G loss : 0.047948]\n",
            "14 [D loss : 1.297815, acc : 37.11%] [G loss : 0.244437]\n",
            "15 [D loss : 1.359809, acc : 34.38%] [G loss : 0.432794]\n",
            "16 [D loss : 1.214233, acc : 36.13%] [G loss : 0.491813]\n",
            "17 [D loss : 0.910094, acc : 51.95%] [G loss : 0.263103]\n",
            "18 [D loss : 0.921351, acc : 46.88%] [G loss : 0.169388]\n",
            "19 [D loss : 1.101434, acc : 37.70%] [G loss : 0.295855]\n",
            "20 [D loss : 1.486670, acc : 27.54%] [G loss : 0.554376]\n",
            "21 [D loss : 1.234111, acc : 34.18%] [G loss : 0.630785]\n",
            "22 [D loss : 1.094154, acc : 44.73%] [G loss : 0.867067]\n",
            "23 [D loss : 0.799954, acc : 56.84%] [G loss : 1.065172]\n",
            "24 [D loss : 0.837474, acc : 54.10%] [G loss : 0.861397]\n",
            "25 [D loss : 0.701833, acc : 59.18%] [G loss : 0.843003]\n",
            "26 [D loss : 0.718784, acc : 60.55%] [G loss : 0.725857]\n",
            "27 [D loss : 1.412719, acc : 49.61%] [G loss : 1.193578]\n",
            "28 [D loss : 1.118115, acc : 49.22%] [G loss : 1.129988]\n",
            "29 [D loss : 1.008643, acc : 49.61%] [G loss : 0.956320]\n",
            "30 [D loss : 0.903765, acc : 52.34%] [G loss : 0.838612]\n",
            "31 [D loss : 0.960998, acc : 50.20%] [G loss : 0.965059]\n",
            "32 [D loss : 0.887829, acc : 50.98%] [G loss : 1.036188]\n",
            "33 [D loss : 0.868323, acc : 53.52%] [G loss : 0.875319]\n",
            "34 [D loss : 0.955779, acc : 51.37%] [G loss : 0.900234]\n",
            "35 [D loss : 0.847620, acc : 53.71%] [G loss : 0.742672]\n",
            "36 [D loss : 0.725165, acc : 58.20%] [G loss : 0.530488]\n",
            "37 [D loss : 1.015682, acc : 51.17%] [G loss : 0.546360]\n",
            "38 [D loss : 1.378083, acc : 46.09%] [G loss : 0.645006]\n",
            "39 [D loss : 1.239662, acc : 37.70%] [G loss : 0.765397]\n",
            "40 [D loss : 1.143293, acc : 42.38%] [G loss : 0.631956]\n",
            "41 [D loss : 1.113325, acc : 42.38%] [G loss : 0.645421]\n",
            "42 [D loss : 1.101430, acc : 41.99%] [G loss : 0.593878]\n",
            "43 [D loss : 1.076000, acc : 42.77%] [G loss : 0.645405]\n",
            "44 [D loss : 1.095797, acc : 41.80%] [G loss : 0.667119]\n",
            "45 [D loss : 1.130568, acc : 41.02%] [G loss : 0.652530]\n",
            "46 [D loss : 1.064045, acc : 43.75%] [G loss : 0.679215]\n",
            "47 [D loss : 1.066676, acc : 41.60%] [G loss : 0.621605]\n",
            "48 [D loss : 1.106964, acc : 43.36%] [G loss : 0.670430]\n",
            "49 [D loss : 1.066831, acc : 43.55%] [G loss : 0.634917]\n",
            "50 [D loss : 1.056348, acc : 41.99%] [G loss : 0.650901]\n",
            "51 [D loss : 1.062815, acc : 42.58%] [G loss : 0.549538]\n",
            "52 [D loss : 1.053795, acc : 42.38%] [G loss : 0.621303]\n",
            "53 [D loss : 1.023162, acc : 43.36%] [G loss : 0.607433]\n",
            "54 [D loss : 1.040016, acc : 45.12%] [G loss : 0.565142]\n",
            "55 [D loss : 1.038761, acc : 41.80%] [G loss : 0.613130]\n",
            "56 [D loss : 1.052106, acc : 41.02%] [G loss : 0.609300]\n",
            "57 [D loss : 1.017591, acc : 45.70%] [G loss : 0.613082]\n",
            "58 [D loss : 1.024152, acc : 43.75%] [G loss : 0.640713]\n",
            "59 [D loss : 1.007590, acc : 43.55%] [G loss : 0.587906]\n",
            "60 [D loss : 1.033359, acc : 43.95%] [G loss : 0.592108]\n",
            "61 [D loss : 0.980471, acc : 44.14%] [G loss : 0.569380]\n",
            "62 [D loss : 0.996914, acc : 43.16%] [G loss : 0.591265]\n",
            "63 [D loss : 0.986954, acc : 44.14%] [G loss : 0.525194]\n",
            "64 [D loss : 1.014542, acc : 43.95%] [G loss : 0.569143]\n",
            "65 [D loss : 0.990100, acc : 42.19%] [G loss : 0.606886]\n",
            "66 [D loss : 1.033500, acc : 41.99%] [G loss : 0.587144]\n",
            "67 [D loss : 0.977865, acc : 45.51%] [G loss : 0.565273]\n",
            "68 [D loss : 0.998033, acc : 45.12%] [G loss : 0.559138]\n",
            "69 [D loss : 0.977738, acc : 43.55%] [G loss : 0.539950]\n",
            "70 [D loss : 0.958603, acc : 42.58%] [G loss : 0.549609]\n",
            "71 [D loss : 0.987664, acc : 43.55%] [G loss : 0.566539]\n",
            "72 [D loss : 1.010244, acc : 45.31%] [G loss : 0.584102]\n",
            "73 [D loss : 0.941037, acc : 46.88%] [G loss : 0.602224]\n",
            "74 [D loss : 0.962041, acc : 44.53%] [G loss : 0.564398]\n",
            "75 [D loss : 0.945046, acc : 47.27%] [G loss : 0.596590]\n",
            "76 [D loss : 0.922422, acc : 46.48%] [G loss : 0.550503]\n",
            "77 [D loss : 0.956207, acc : 45.12%] [G loss : 0.544199]\n",
            "78 [D loss : 0.971329, acc : 45.12%] [G loss : 0.569778]\n",
            "79 [D loss : 0.938849, acc : 46.88%] [G loss : 0.571132]\n",
            "80 [D loss : 0.918624, acc : 47.46%] [G loss : 0.568028]\n",
            "81 [D loss : 0.960627, acc : 45.12%] [G loss : 0.566279]\n",
            "82 [D loss : 0.899419, acc : 47.27%] [G loss : 0.565059]\n",
            "83 [D loss : 0.951227, acc : 48.24%] [G loss : 0.601195]\n",
            "84 [D loss : 0.966125, acc : 46.09%] [G loss : 0.531584]\n",
            "85 [D loss : 0.964301, acc : 44.53%] [G loss : 0.565067]\n",
            "86 [D loss : 0.948091, acc : 42.97%] [G loss : 0.581522]\n",
            "87 [D loss : 1.011526, acc : 43.75%] [G loss : 0.509284]\n",
            "88 [D loss : 0.979223, acc : 43.95%] [G loss : 0.564211]\n",
            "89 [D loss : 0.963852, acc : 45.51%] [G loss : 0.542716]\n",
            "90 [D loss : 0.977307, acc : 44.34%] [G loss : 0.521784]\n",
            "91 [D loss : 0.967716, acc : 41.02%] [G loss : 0.568123]\n",
            "92 [D loss : 0.957233, acc : 44.14%] [G loss : 0.486372]\n",
            "93 [D loss : 0.949008, acc : 45.31%] [G loss : 0.551238]\n",
            "94 [D loss : 0.936681, acc : 45.51%] [G loss : 0.540908]\n",
            "95 [D loss : 0.975960, acc : 42.97%] [G loss : 0.484914]\n",
            "96 [D loss : 0.983316, acc : 45.51%] [G loss : 0.560678]\n",
            "97 [D loss : 0.983127, acc : 42.97%] [G loss : 0.543562]\n",
            "98 [D loss : 1.005446, acc : 42.38%] [G loss : 0.553172]\n",
            "99 [D loss : 0.993822, acc : 43.95%] [G loss : 0.560135]\n",
            "100 [D loss : 0.955922, acc : 43.55%] [G loss : 0.558365]\n",
            "101 [D loss : 0.941992, acc : 45.31%] [G loss : 0.551036]\n",
            "102 [D loss : 0.941958, acc : 46.09%] [G loss : 0.544706]\n",
            "103 [D loss : 0.905480, acc : 45.90%] [G loss : 0.572889]\n",
            "104 [D loss : 0.980193, acc : 44.92%] [G loss : 0.506159]\n",
            "105 [D loss : 0.945192, acc : 46.09%] [G loss : 0.560108]\n",
            "106 [D loss : 0.906071, acc : 48.44%] [G loss : 0.539978]\n",
            "107 [D loss : 0.976441, acc : 45.90%] [G loss : 0.517783]\n",
            "108 [D loss : 0.928803, acc : 45.12%] [G loss : 0.536390]\n",
            "109 [D loss : 0.940909, acc : 43.95%] [G loss : 0.553886]\n",
            "110 [D loss : 0.937096, acc : 42.97%] [G loss : 0.557055]\n",
            "111 [D loss : 0.926401, acc : 46.68%] [G loss : 0.527536]\n",
            "112 [D loss : 0.913298, acc : 43.55%] [G loss : 0.513129]\n",
            "113 [D loss : 0.945370, acc : 44.34%] [G loss : 0.510068]\n",
            "114 [D loss : 0.935353, acc : 47.07%] [G loss : 0.529381]\n",
            "115 [D loss : 0.969517, acc : 44.34%] [G loss : 0.528850]\n",
            "116 [D loss : 0.943633, acc : 44.53%] [G loss : 0.527337]\n",
            "117 [D loss : 0.954011, acc : 46.29%] [G loss : 0.500102]\n",
            "118 [D loss : 0.956224, acc : 44.34%] [G loss : 0.518368]\n",
            "119 [D loss : 0.965016, acc : 43.16%] [G loss : 0.528876]\n",
            "120 [D loss : 0.923223, acc : 44.92%] [G loss : 0.518068]\n",
            "121 [D loss : 0.963613, acc : 44.34%] [G loss : 0.510687]\n",
            "122 [D loss : 0.960050, acc : 44.14%] [G loss : 0.502394]\n",
            "123 [D loss : 0.947809, acc : 44.14%] [G loss : 0.499593]\n",
            "124 [D loss : 0.940731, acc : 44.92%] [G loss : 0.574852]\n",
            "125 [D loss : 0.949347, acc : 44.73%] [G loss : 0.537158]\n",
            "126 [D loss : 0.949620, acc : 44.14%] [G loss : 0.481213]\n",
            "127 [D loss : 0.954445, acc : 45.90%] [G loss : 0.495284]\n",
            "128 [D loss : 0.946088, acc : 45.31%] [G loss : 0.530591]\n",
            "129 [D loss : 0.934314, acc : 45.90%] [G loss : 0.525602]\n",
            "130 [D loss : 0.891751, acc : 47.27%] [G loss : 0.554943]\n",
            "131 [D loss : 0.972660, acc : 43.95%] [G loss : 0.561906]\n",
            "132 [D loss : 0.947138, acc : 43.95%] [G loss : 0.519326]\n",
            "133 [D loss : 0.935393, acc : 45.70%] [G loss : 0.508500]\n",
            "134 [D loss : 0.913204, acc : 47.07%] [G loss : 0.542644]\n",
            "135 [D loss : 0.949768, acc : 45.31%] [G loss : 0.511840]\n",
            "136 [D loss : 0.913892, acc : 47.46%] [G loss : 0.527153]\n",
            "137 [D loss : 0.926347, acc : 44.92%] [G loss : 0.527452]\n",
            "138 [D loss : 0.911300, acc : 45.12%] [G loss : 0.531487]\n",
            "139 [D loss : 0.959972, acc : 43.95%] [G loss : 0.503315]\n",
            "140 [D loss : 0.920669, acc : 43.75%] [G loss : 0.501267]\n",
            "141 [D loss : 0.889528, acc : 46.68%] [G loss : 0.510966]\n",
            "142 [D loss : 0.927185, acc : 44.53%] [G loss : 0.514080]\n",
            "143 [D loss : 0.936467, acc : 45.90%] [G loss : 0.509263]\n",
            "144 [D loss : 0.913183, acc : 44.53%] [G loss : 0.552967]\n",
            "145 [D loss : 0.939150, acc : 44.53%] [G loss : 0.477104]\n",
            "146 [D loss : 0.914565, acc : 48.24%] [G loss : 0.531558]\n",
            "147 [D loss : 0.929541, acc : 44.53%] [G loss : 0.513613]\n",
            "148 [D loss : 0.902982, acc : 45.12%] [G loss : 0.548541]\n",
            "149 [D loss : 0.883173, acc : 46.68%] [G loss : 0.540983]\n",
            "150 [D loss : 0.878559, acc : 48.44%] [G loss : 0.508729]\n",
            "151 [D loss : 0.918713, acc : 46.68%] [G loss : 0.530370]\n",
            "152 [D loss : 0.910949, acc : 45.70%] [G loss : 0.496219]\n",
            "153 [D loss : 0.930156, acc : 46.48%] [G loss : 0.542421]\n",
            "154 [D loss : 0.912785, acc : 44.53%] [G loss : 0.490373]\n",
            "155 [D loss : 0.950566, acc : 45.51%] [G loss : 0.513245]\n",
            "156 [D loss : 0.921481, acc : 43.16%] [G loss : 0.505613]\n",
            "157 [D loss : 0.888993, acc : 45.12%] [G loss : 0.505151]\n",
            "158 [D loss : 0.930396, acc : 46.88%] [G loss : 0.530014]\n",
            "159 [D loss : 0.906299, acc : 46.88%] [G loss : 0.521801]\n",
            "160 [D loss : 0.914310, acc : 47.27%] [G loss : 0.501302]\n",
            "161 [D loss : 0.863543, acc : 47.85%] [G loss : 0.537928]\n",
            "162 [D loss : 0.939258, acc : 45.90%] [G loss : 0.509733]\n",
            "163 [D loss : 0.890958, acc : 47.07%] [G loss : 0.511992]\n",
            "164 [D loss : 0.896737, acc : 46.88%] [G loss : 0.569775]\n",
            "165 [D loss : 0.915859, acc : 43.95%] [G loss : 0.531651]\n",
            "166 [D loss : 0.928136, acc : 44.14%] [G loss : 0.477868]\n",
            "167 [D loss : 0.863167, acc : 46.48%] [G loss : 0.520388]\n",
            "168 [D loss : 0.895114, acc : 47.27%] [G loss : 0.523791]\n",
            "169 [D loss : 0.947178, acc : 45.12%] [G loss : 0.493146]\n",
            "170 [D loss : 0.932499, acc : 44.92%] [G loss : 0.485704]\n",
            "171 [D loss : 0.910155, acc : 44.92%] [G loss : 0.532128]\n",
            "172 [D loss : 0.892155, acc : 47.85%] [G loss : 0.521506]\n",
            "173 [D loss : 0.905437, acc : 45.12%] [G loss : 0.511485]\n",
            "174 [D loss : 0.907022, acc : 47.66%] [G loss : 0.476245]\n",
            "175 [D loss : 0.910005, acc : 47.85%] [G loss : 0.512982]\n",
            "176 [D loss : 0.920090, acc : 48.05%] [G loss : 0.505473]\n",
            "177 [D loss : 0.922333, acc : 44.92%] [G loss : 0.534374]\n",
            "178 [D loss : 0.923646, acc : 46.09%] [G loss : 0.521397]\n",
            "179 [D loss : 0.887822, acc : 46.09%] [G loss : 0.496039]\n",
            "180 [D loss : 0.877109, acc : 50.00%] [G loss : 0.529567]\n",
            "181 [D loss : 0.903166, acc : 46.48%] [G loss : 0.505173]\n",
            "182 [D loss : 0.883300, acc : 46.68%] [G loss : 0.534464]\n",
            "183 [D loss : 0.911318, acc : 48.05%] [G loss : 0.533557]\n",
            "184 [D loss : 0.896619, acc : 47.46%] [G loss : 0.521842]\n",
            "185 [D loss : 0.926666, acc : 44.34%] [G loss : 0.486800]\n",
            "186 [D loss : 0.919708, acc : 45.31%] [G loss : 0.507286]\n",
            "187 [D loss : 0.920898, acc : 44.73%] [G loss : 0.527508]\n",
            "188 [D loss : 0.916394, acc : 42.97%] [G loss : 0.524976]\n",
            "189 [D loss : 0.883080, acc : 48.44%] [G loss : 0.516797]\n",
            "190 [D loss : 0.903257, acc : 47.85%] [G loss : 0.496228]\n",
            "191 [D loss : 0.924054, acc : 47.07%] [G loss : 0.539052]\n",
            "192 [D loss : 0.925200, acc : 46.68%] [G loss : 0.517605]\n",
            "193 [D loss : 0.889053, acc : 47.27%] [G loss : 0.500745]\n",
            "194 [D loss : 0.903675, acc : 48.24%] [G loss : 0.511759]\n",
            "195 [D loss : 0.861179, acc : 47.46%] [G loss : 0.486653]\n",
            "196 [D loss : 0.883504, acc : 48.44%] [G loss : 0.535682]\n",
            "197 [D loss : 0.883634, acc : 45.51%] [G loss : 0.504960]\n",
            "198 [D loss : 0.881407, acc : 47.46%] [G loss : 0.500136]\n",
            "199 [D loss : 0.906361, acc : 47.07%] [G loss : 0.489009]\n",
            "200 [D loss : 0.931199, acc : 45.51%] [G loss : 0.501714]\n",
            "201 [D loss : 0.910934, acc : 44.73%] [G loss : 0.510303]\n",
            "202 [D loss : 0.898586, acc : 46.68%] [G loss : 0.493027]\n",
            "203 [D loss : 0.852249, acc : 48.83%] [G loss : 0.526148]\n",
            "204 [D loss : 0.897080, acc : 46.88%] [G loss : 0.520708]\n",
            "205 [D loss : 0.878656, acc : 46.68%] [G loss : 0.502512]\n",
            "206 [D loss : 0.911750, acc : 45.31%] [G loss : 0.515991]\n",
            "207 [D loss : 0.900626, acc : 46.09%] [G loss : 0.509410]\n",
            "208 [D loss : 0.891212, acc : 45.70%] [G loss : 0.504277]\n",
            "209 [D loss : 0.903693, acc : 46.48%] [G loss : 0.506965]\n",
            "210 [D loss : 0.867026, acc : 45.51%] [G loss : 0.512645]\n",
            "211 [D loss : 0.892030, acc : 47.46%] [G loss : 0.511838]\n",
            "212 [D loss : 0.923985, acc : 46.29%] [G loss : 0.469652]\n",
            "213 [D loss : 0.899366, acc : 46.48%] [G loss : 0.505299]\n",
            "214 [D loss : 0.884528, acc : 46.48%] [G loss : 0.474914]\n",
            "215 [D loss : 0.928672, acc : 46.68%] [G loss : 0.489058]\n",
            "216 [D loss : 0.905353, acc : 48.24%] [G loss : 0.523798]\n",
            "217 [D loss : 0.867706, acc : 49.80%] [G loss : 0.517664]\n",
            "218 [D loss : 0.909793, acc : 46.48%] [G loss : 0.476494]\n",
            "219 [D loss : 0.894372, acc : 46.68%] [G loss : 0.492970]\n",
            "220 [D loss : 0.898890, acc : 44.34%] [G loss : 0.514363]\n",
            "221 [D loss : 0.906419, acc : 46.09%] [G loss : 0.525813]\n",
            "222 [D loss : 0.911269, acc : 44.92%] [G loss : 0.518894]\n",
            "223 [D loss : 0.886006, acc : 50.20%] [G loss : 0.509204]\n",
            "224 [D loss : 0.869404, acc : 49.41%] [G loss : 0.474086]\n",
            "225 [D loss : 0.862769, acc : 45.90%] [G loss : 0.515607]\n",
            "226 [D loss : 0.900551, acc : 47.27%] [G loss : 0.497629]\n",
            "227 [D loss : 0.895187, acc : 48.05%] [G loss : 0.485439]\n",
            "228 [D loss : 0.886907, acc : 45.70%] [G loss : 0.508603]\n",
            "229 [D loss : 0.863296, acc : 47.85%] [G loss : 0.496038]\n",
            "230 [D loss : 0.874977, acc : 46.29%] [G loss : 0.488555]\n",
            "231 [D loss : 0.931003, acc : 46.88%] [G loss : 0.474303]\n",
            "232 [D loss : 0.901167, acc : 43.95%] [G loss : 0.498534]\n",
            "233 [D loss : 0.916177, acc : 45.51%] [G loss : 0.486569]\n",
            "234 [D loss : 0.893002, acc : 45.31%] [G loss : 0.513310]\n",
            "235 [D loss : 0.907510, acc : 43.95%] [G loss : 0.523003]\n",
            "236 [D loss : 0.885726, acc : 45.90%] [G loss : 0.509757]\n",
            "237 [D loss : 0.864027, acc : 48.24%] [G loss : 0.513626]\n",
            "238 [D loss : 0.880469, acc : 46.29%] [G loss : 0.541763]\n",
            "239 [D loss : 0.877086, acc : 49.41%] [G loss : 0.478269]\n",
            "240 [D loss : 0.892039, acc : 48.05%] [G loss : 0.508968]\n",
            "241 [D loss : 0.878201, acc : 47.66%] [G loss : 0.499403]\n",
            "242 [D loss : 0.895844, acc : 47.27%] [G loss : 0.455802]\n",
            "243 [D loss : 0.874569, acc : 47.85%] [G loss : 0.507499]\n",
            "244 [D loss : 0.881844, acc : 50.00%] [G loss : 0.523643]\n",
            "245 [D loss : 0.891044, acc : 45.90%] [G loss : 0.507038]\n",
            "246 [D loss : 0.910594, acc : 46.29%] [G loss : 0.490492]\n",
            "247 [D loss : 0.894716, acc : 46.29%] [G loss : 0.512224]\n",
            "248 [D loss : 0.886184, acc : 47.85%] [G loss : 0.476474]\n",
            "249 [D loss : 0.855477, acc : 46.88%] [G loss : 0.504630]\n",
            "250 [D loss : 0.916579, acc : 46.88%] [G loss : 0.505114]\n",
            "251 [D loss : 0.856136, acc : 45.51%] [G loss : 0.507093]\n",
            "252 [D loss : 0.851148, acc : 49.22%] [G loss : 0.493389]\n",
            "253 [D loss : 0.882690, acc : 47.85%] [G loss : 0.524124]\n",
            "254 [D loss : 0.887118, acc : 46.68%] [G loss : 0.492541]\n",
            "255 [D loss : 0.861003, acc : 47.46%] [G loss : 0.496360]\n",
            "256 [D loss : 0.864103, acc : 48.63%] [G loss : 0.461287]\n",
            "257 [D loss : 0.872942, acc : 46.88%] [G loss : 0.501776]\n",
            "258 [D loss : 0.870982, acc : 47.27%] [G loss : 0.495801]\n",
            "259 [D loss : 0.865751, acc : 47.27%] [G loss : 0.525561]\n",
            "260 [D loss : 0.936443, acc : 44.92%] [G loss : 0.488761]\n",
            "261 [D loss : 0.878700, acc : 44.14%] [G loss : 0.483576]\n",
            "262 [D loss : 0.922664, acc : 44.73%] [G loss : 0.483856]\n",
            "263 [D loss : 0.916024, acc : 45.12%] [G loss : 0.523764]\n",
            "264 [D loss : 0.890554, acc : 45.90%] [G loss : 0.478134]\n",
            "265 [D loss : 0.896561, acc : 45.90%] [G loss : 0.498372]\n",
            "266 [D loss : 0.874435, acc : 46.88%] [G loss : 0.526302]\n",
            "267 [D loss : 0.875315, acc : 45.70%] [G loss : 0.487238]\n",
            "268 [D loss : 0.851398, acc : 49.02%] [G loss : 0.495254]\n",
            "269 [D loss : 0.852460, acc : 48.63%] [G loss : 0.513529]\n",
            "270 [D loss : 0.862372, acc : 49.02%] [G loss : 0.497067]\n",
            "271 [D loss : 0.879993, acc : 46.29%] [G loss : 0.515783]\n",
            "272 [D loss : 0.892543, acc : 48.44%] [G loss : 0.512326]\n",
            "273 [D loss : 0.884946, acc : 48.05%] [G loss : 0.475871]\n",
            "274 [D loss : 0.877527, acc : 47.46%] [G loss : 0.483005]\n",
            "275 [D loss : 0.855495, acc : 46.88%] [G loss : 0.466943]\n",
            "276 [D loss : 0.888883, acc : 47.66%] [G loss : 0.474476]\n",
            "277 [D loss : 0.883850, acc : 45.70%] [G loss : 0.496899]\n",
            "278 [D loss : 0.927080, acc : 44.53%] [G loss : 0.487815]\n",
            "279 [D loss : 0.896681, acc : 49.41%] [G loss : 0.488564]\n",
            "280 [D loss : 0.875028, acc : 47.27%] [G loss : 0.497227]\n",
            "281 [D loss : 0.902186, acc : 46.48%] [G loss : 0.516586]\n",
            "282 [D loss : 0.887380, acc : 46.68%] [G loss : 0.503614]\n",
            "283 [D loss : 0.841765, acc : 48.63%] [G loss : 0.526431]\n",
            "284 [D loss : 0.866106, acc : 48.24%] [G loss : 0.482746]\n",
            "285 [D loss : 0.886221, acc : 46.88%] [G loss : 0.501164]\n",
            "286 [D loss : 0.894070, acc : 47.07%] [G loss : 0.516594]\n",
            "287 [D loss : 0.898618, acc : 47.07%] [G loss : 0.495403]\n",
            "288 [D loss : 0.895041, acc : 45.31%] [G loss : 0.488865]\n",
            "289 [D loss : 0.888455, acc : 45.12%] [G loss : 0.509232]\n",
            "290 [D loss : 0.886384, acc : 46.48%] [G loss : 0.504405]\n",
            "291 [D loss : 0.877982, acc : 46.88%] [G loss : 0.516883]\n",
            "292 [D loss : 0.896636, acc : 46.09%] [G loss : 0.497200]\n",
            "293 [D loss : 0.847252, acc : 49.02%] [G loss : 0.501002]\n",
            "294 [D loss : 0.897688, acc : 46.09%] [G loss : 0.495963]\n",
            "295 [D loss : 0.861990, acc : 47.07%] [G loss : 0.478054]\n",
            "296 [D loss : 0.851801, acc : 46.09%] [G loss : 0.492507]\n",
            "297 [D loss : 0.892308, acc : 45.51%] [G loss : 0.499724]\n",
            "298 [D loss : 0.885877, acc : 46.29%] [G loss : 0.471445]\n",
            "299 [D loss : 0.864949, acc : 45.70%] [G loss : 0.484330]\n",
            "300 [D loss : 0.883732, acc : 46.48%] [G loss : 0.492197]\n",
            "301 [D loss : 0.875260, acc : 47.85%] [G loss : 0.540679]\n",
            "302 [D loss : 0.905483, acc : 46.09%] [G loss : 0.497132]\n",
            "303 [D loss : 0.903201, acc : 45.12%] [G loss : 0.496591]\n",
            "304 [D loss : 0.874402, acc : 47.85%] [G loss : 0.511534]\n",
            "305 [D loss : 0.873535, acc : 48.05%] [G loss : 0.525948]\n",
            "306 [D loss : 0.879513, acc : 45.31%] [G loss : 0.501613]\n",
            "307 [D loss : 0.881840, acc : 47.46%] [G loss : 0.494462]\n",
            "308 [D loss : 0.866168, acc : 48.05%] [G loss : 0.505711]\n",
            "309 [D loss : 0.891710, acc : 46.88%] [G loss : 0.470601]\n",
            "310 [D loss : 0.881566, acc : 46.29%] [G loss : 0.459509]\n",
            "311 [D loss : 0.862168, acc : 47.27%] [G loss : 0.485663]\n",
            "312 [D loss : 0.843728, acc : 47.66%] [G loss : 0.494842]\n",
            "313 [D loss : 0.871764, acc : 48.83%] [G loss : 0.508541]\n",
            "314 [D loss : 0.839078, acc : 46.29%] [G loss : 0.509615]\n",
            "315 [D loss : 0.843808, acc : 50.59%] [G loss : 0.489129]\n",
            "316 [D loss : 0.860133, acc : 48.05%] [G loss : 0.496788]\n",
            "317 [D loss : 0.881993, acc : 48.05%] [G loss : 0.474341]\n",
            "318 [D loss : 0.854414, acc : 48.63%] [G loss : 0.506599]\n",
            "319 [D loss : 0.876131, acc : 47.07%] [G loss : 0.481774]\n",
            "320 [D loss : 0.871991, acc : 46.88%] [G loss : 0.492941]\n",
            "321 [D loss : 0.872355, acc : 46.29%] [G loss : 0.498480]\n",
            "322 [D loss : 0.881252, acc : 47.85%] [G loss : 0.471888]\n",
            "323 [D loss : 0.875110, acc : 45.31%] [G loss : 0.487018]\n",
            "324 [D loss : 0.867545, acc : 48.05%] [G loss : 0.489663]\n",
            "325 [D loss : 0.898292, acc : 45.12%] [G loss : 0.466270]\n",
            "326 [D loss : 0.855641, acc : 47.85%] [G loss : 0.500611]\n",
            "327 [D loss : 0.882929, acc : 48.24%] [G loss : 0.476118]\n",
            "328 [D loss : 0.896500, acc : 47.27%] [G loss : 0.523423]\n",
            "329 [D loss : 0.861246, acc : 47.07%] [G loss : 0.497445]\n",
            "330 [D loss : 0.915779, acc : 44.53%] [G loss : 0.466594]\n",
            "331 [D loss : 0.892455, acc : 46.09%] [G loss : 0.504665]\n",
            "332 [D loss : 0.870165, acc : 48.63%] [G loss : 0.490754]\n",
            "333 [D loss : 0.881143, acc : 47.46%] [G loss : 0.451031]\n",
            "334 [D loss : 0.868454, acc : 48.24%] [G loss : 0.500864]\n",
            "335 [D loss : 0.880146, acc : 45.51%] [G loss : 0.493305]\n",
            "336 [D loss : 0.844921, acc : 49.80%] [G loss : 0.513556]\n",
            "337 [D loss : 0.858400, acc : 46.68%] [G loss : 0.497453]\n",
            "338 [D loss : 0.871458, acc : 48.05%] [G loss : 0.479896]\n",
            "339 [D loss : 0.855101, acc : 48.05%] [G loss : 0.487173]\n",
            "340 [D loss : 0.840564, acc : 48.24%] [G loss : 0.503033]\n",
            "341 [D loss : 0.868613, acc : 48.44%] [G loss : 0.488122]\n",
            "342 [D loss : 0.844987, acc : 47.85%] [G loss : 0.497905]\n",
            "343 [D loss : 0.857960, acc : 48.05%] [G loss : 0.490166]\n",
            "344 [D loss : 0.844095, acc : 48.44%] [G loss : 0.488361]\n",
            "345 [D loss : 0.871364, acc : 45.70%] [G loss : 0.488380]\n",
            "346 [D loss : 0.847533, acc : 48.83%] [G loss : 0.487482]\n",
            "347 [D loss : 0.852374, acc : 49.02%] [G loss : 0.487027]\n",
            "348 [D loss : 0.866269, acc : 46.88%] [G loss : 0.467289]\n",
            "349 [D loss : 0.894147, acc : 45.90%] [G loss : 0.487396]\n",
            "350 [D loss : 0.895899, acc : 45.90%] [G loss : 0.457195]\n",
            "351 [D loss : 0.846701, acc : 49.22%] [G loss : 0.491803]\n",
            "352 [D loss : 0.880852, acc : 45.90%] [G loss : 0.484219]\n",
            "353 [D loss : 0.837808, acc : 48.05%] [G loss : 0.505988]\n",
            "354 [D loss : 0.885970, acc : 48.44%] [G loss : 0.484625]\n",
            "355 [D loss : 0.888882, acc : 46.29%] [G loss : 0.497127]\n",
            "356 [D loss : 0.880220, acc : 48.44%] [G loss : 0.484960]\n",
            "357 [D loss : 0.878714, acc : 45.12%] [G loss : 0.503199]\n",
            "358 [D loss : 0.879266, acc : 47.46%] [G loss : 0.517566]\n",
            "359 [D loss : 0.868623, acc : 47.07%] [G loss : 0.470381]\n",
            "360 [D loss : 0.835592, acc : 49.22%] [G loss : 0.490358]\n",
            "361 [D loss : 0.888859, acc : 44.92%] [G loss : 0.491978]\n",
            "362 [D loss : 0.864105, acc : 47.27%] [G loss : 0.473437]\n",
            "363 [D loss : 0.856578, acc : 47.27%] [G loss : 0.493293]\n",
            "364 [D loss : 0.843520, acc : 47.46%] [G loss : 0.463555]\n",
            "365 [D loss : 0.867758, acc : 47.46%] [G loss : 0.472794]\n",
            "366 [D loss : 0.845549, acc : 48.24%] [G loss : 0.477108]\n",
            "367 [D loss : 0.872467, acc : 47.27%] [G loss : 0.493684]\n",
            "368 [D loss : 0.852349, acc : 48.83%] [G loss : 0.482077]\n",
            "369 [D loss : 0.856972, acc : 46.68%] [G loss : 0.509446]\n",
            "370 [D loss : 0.835490, acc : 48.63%] [G loss : 0.490273]\n",
            "371 [D loss : 0.836298, acc : 48.83%] [G loss : 0.490415]\n",
            "372 [D loss : 0.856353, acc : 48.44%] [G loss : 0.527763]\n",
            "373 [D loss : 0.862487, acc : 47.85%] [G loss : 0.484967]\n",
            "374 [D loss : 0.850983, acc : 47.85%] [G loss : 0.496953]\n",
            "375 [D loss : 0.845207, acc : 48.24%] [G loss : 0.499048]\n",
            "376 [D loss : 0.852595, acc : 46.88%] [G loss : 0.481263]\n",
            "377 [D loss : 0.868089, acc : 47.46%] [G loss : 0.468285]\n",
            "378 [D loss : 0.867684, acc : 48.24%] [G loss : 0.481660]\n",
            "379 [D loss : 0.849222, acc : 48.24%] [G loss : 0.520317]\n",
            "380 [D loss : 0.863200, acc : 48.44%] [G loss : 0.490048]\n",
            "381 [D loss : 0.864908, acc : 45.90%] [G loss : 0.473956]\n",
            "382 [D loss : 0.865219, acc : 47.46%] [G loss : 0.501831]\n",
            "383 [D loss : 0.884586, acc : 47.27%] [G loss : 0.475304]\n",
            "384 [D loss : 0.881445, acc : 47.66%] [G loss : 0.484771]\n",
            "385 [D loss : 0.861532, acc : 49.41%] [G loss : 0.499899]\n",
            "386 [D loss : 0.828362, acc : 48.44%] [G loss : 0.518884]\n",
            "387 [D loss : 0.842404, acc : 47.27%] [G loss : 0.516202]\n",
            "388 [D loss : 0.855623, acc : 47.66%] [G loss : 0.502420]\n",
            "389 [D loss : 0.828313, acc : 49.41%] [G loss : 0.506611]\n",
            "390 [D loss : 0.850688, acc : 46.29%] [G loss : 0.474805]\n",
            "391 [D loss : 0.857419, acc : 46.88%] [G loss : 0.504665]\n",
            "392 [D loss : 0.854348, acc : 46.68%] [G loss : 0.463608]\n",
            "393 [D loss : 0.901080, acc : 45.12%] [G loss : 0.487320]\n",
            "394 [D loss : 0.846978, acc : 49.41%] [G loss : 0.472370]\n",
            "395 [D loss : 0.849884, acc : 48.44%] [G loss : 0.525031]\n",
            "396 [D loss : 0.861312, acc : 47.66%] [G loss : 0.495461]\n",
            "397 [D loss : 0.818750, acc : 50.39%] [G loss : 0.488818]\n",
            "398 [D loss : 0.873123, acc : 49.22%] [G loss : 0.481533]\n",
            "399 [D loss : 0.849210, acc : 48.44%] [G loss : 0.487627]\n",
            "400 [D loss : 0.854773, acc : 47.27%] [G loss : 0.495181]\n",
            "401 [D loss : 0.883881, acc : 48.24%] [G loss : 0.476341]\n",
            "402 [D loss : 0.857517, acc : 48.24%] [G loss : 0.489104]\n",
            "403 [D loss : 0.850951, acc : 47.66%] [G loss : 0.537429]\n",
            "404 [D loss : 0.852337, acc : 45.90%] [G loss : 0.523554]\n",
            "405 [D loss : 0.852655, acc : 47.07%] [G loss : 0.482083]\n",
            "406 [D loss : 0.862499, acc : 48.44%] [G loss : 0.497036]\n",
            "407 [D loss : 0.864077, acc : 47.85%] [G loss : 0.472585]\n",
            "408 [D loss : 0.842345, acc : 48.05%] [G loss : 0.487606]\n",
            "409 [D loss : 0.848358, acc : 48.24%] [G loss : 0.468238]\n",
            "410 [D loss : 0.807106, acc : 50.59%] [G loss : 0.519295]\n",
            "411 [D loss : 0.842281, acc : 48.44%] [G loss : 0.522742]\n",
            "412 [D loss : 0.834736, acc : 48.05%] [G loss : 0.472556]\n",
            "413 [D loss : 0.832753, acc : 49.41%] [G loss : 0.488868]\n",
            "414 [D loss : 0.834326, acc : 49.80%] [G loss : 0.490178]\n",
            "415 [D loss : 0.851155, acc : 48.83%] [G loss : 0.463655]\n",
            "416 [D loss : 0.851540, acc : 47.46%] [G loss : 0.506640]\n",
            "417 [D loss : 0.879449, acc : 46.48%] [G loss : 0.464115]\n",
            "418 [D loss : 0.858486, acc : 48.05%] [G loss : 0.485164]\n",
            "419 [D loss : 0.872558, acc : 44.73%] [G loss : 0.490491]\n",
            "420 [D loss : 0.831365, acc : 49.22%] [G loss : 0.505542]\n",
            "421 [D loss : 0.851310, acc : 48.44%] [G loss : 0.483413]\n",
            "422 [D loss : 0.871085, acc : 47.07%] [G loss : 0.519629]\n",
            "423 [D loss : 0.852183, acc : 48.83%] [G loss : 0.484542]\n",
            "424 [D loss : 0.865849, acc : 47.27%] [G loss : 0.488071]\n",
            "425 [D loss : 0.863654, acc : 49.41%] [G loss : 0.499844]\n",
            "426 [D loss : 0.841086, acc : 48.05%] [G loss : 0.504631]\n",
            "427 [D loss : 0.829856, acc : 49.02%] [G loss : 0.473223]\n",
            "428 [D loss : 0.846440, acc : 46.68%] [G loss : 0.471859]\n",
            "429 [D loss : 0.848943, acc : 47.66%] [G loss : 0.477450]\n",
            "430 [D loss : 0.845020, acc : 48.63%] [G loss : 0.491045]\n",
            "431 [D loss : 0.844863, acc : 48.44%] [G loss : 0.486172]\n",
            "432 [D loss : 0.854503, acc : 46.09%] [G loss : 0.500987]\n",
            "433 [D loss : 0.868929, acc : 47.85%] [G loss : 0.496313]\n",
            "434 [D loss : 0.835502, acc : 49.41%] [G loss : 0.502162]\n",
            "435 [D loss : 0.840336, acc : 49.41%] [G loss : 0.486246]\n",
            "436 [D loss : 0.833296, acc : 48.63%] [G loss : 0.461342]\n",
            "437 [D loss : 0.880605, acc : 47.27%] [G loss : 0.468878]\n",
            "438 [D loss : 0.861993, acc : 47.66%] [G loss : 0.465275]\n",
            "439 [D loss : 0.901188, acc : 44.92%] [G loss : 0.496880]\n",
            "440 [D loss : 0.873091, acc : 47.85%] [G loss : 0.485821]\n",
            "441 [D loss : 0.854992, acc : 48.24%] [G loss : 0.482820]\n",
            "442 [D loss : 0.858378, acc : 48.05%] [G loss : 0.517400]\n",
            "443 [D loss : 0.856075, acc : 47.66%] [G loss : 0.494937]\n",
            "444 [D loss : 0.847559, acc : 47.66%] [G loss : 0.509971]\n",
            "445 [D loss : 0.866400, acc : 46.29%] [G loss : 0.505459]\n",
            "446 [D loss : 0.839978, acc : 47.66%] [G loss : 0.485288]\n",
            "447 [D loss : 0.866136, acc : 48.05%] [G loss : 0.496490]\n",
            "448 [D loss : 0.848888, acc : 48.83%] [G loss : 0.469647]\n",
            "449 [D loss : 0.842128, acc : 49.02%] [G loss : 0.471019]\n",
            "450 [D loss : 0.840995, acc : 47.27%] [G loss : 0.472236]\n",
            "451 [D loss : 0.821982, acc : 48.44%] [G loss : 0.481316]\n",
            "452 [D loss : 0.859330, acc : 47.85%] [G loss : 0.472519]\n",
            "453 [D loss : 0.859700, acc : 48.05%] [G loss : 0.464206]\n",
            "454 [D loss : 0.844753, acc : 47.27%] [G loss : 0.462983]\n",
            "455 [D loss : 0.839043, acc : 49.41%] [G loss : 0.474899]\n",
            "456 [D loss : 0.856985, acc : 48.63%] [G loss : 0.476160]\n",
            "457 [D loss : 0.856987, acc : 49.02%] [G loss : 0.498249]\n",
            "458 [D loss : 0.850026, acc : 47.85%] [G loss : 0.491629]\n",
            "459 [D loss : 0.866661, acc : 46.68%] [G loss : 0.501360]\n",
            "460 [D loss : 0.853831, acc : 48.83%] [G loss : 0.495529]\n",
            "461 [D loss : 0.836453, acc : 48.44%] [G loss : 0.475881]\n",
            "462 [D loss : 0.848621, acc : 47.27%] [G loss : 0.488198]\n",
            "463 [D loss : 0.858672, acc : 48.44%] [G loss : 0.475446]\n",
            "464 [D loss : 0.852306, acc : 46.88%] [G loss : 0.492633]\n",
            "465 [D loss : 0.864786, acc : 45.90%] [G loss : 0.488948]\n",
            "466 [D loss : 0.840427, acc : 47.46%] [G loss : 0.483110]\n",
            "467 [D loss : 0.819247, acc : 48.44%] [G loss : 0.480681]\n",
            "468 [D loss : 0.818199, acc : 49.80%] [G loss : 0.508203]\n",
            "469 [D loss : 0.855406, acc : 48.05%] [G loss : 0.518867]\n",
            "470 [D loss : 0.853271, acc : 47.46%] [G loss : 0.456722]\n",
            "471 [D loss : 0.827634, acc : 50.59%] [G loss : 0.515284]\n",
            "472 [D loss : 0.862650, acc : 47.27%] [G loss : 0.490708]\n",
            "473 [D loss : 0.841072, acc : 48.44%] [G loss : 0.501059]\n",
            "474 [D loss : 0.859234, acc : 47.85%] [G loss : 0.481473]\n",
            "475 [D loss : 0.847555, acc : 48.05%] [G loss : 0.465765]\n",
            "476 [D loss : 0.827874, acc : 47.27%] [G loss : 0.474652]\n",
            "477 [D loss : 0.862515, acc : 47.85%] [G loss : 0.459199]\n",
            "478 [D loss : 0.857363, acc : 46.88%] [G loss : 0.491534]\n",
            "479 [D loss : 0.849211, acc : 47.07%] [G loss : 0.481092]\n",
            "480 [D loss : 0.864343, acc : 49.41%] [G loss : 0.482449]\n",
            "481 [D loss : 0.852724, acc : 48.24%] [G loss : 0.487437]\n",
            "482 [D loss : 0.856457, acc : 48.44%] [G loss : 0.486760]\n",
            "483 [D loss : 0.849716, acc : 49.02%] [G loss : 0.476150]\n",
            "484 [D loss : 0.882587, acc : 45.51%] [G loss : 0.472205]\n",
            "485 [D loss : 0.833697, acc : 49.22%] [G loss : 0.487703]\n",
            "486 [D loss : 0.842192, acc : 48.44%] [G loss : 0.495645]\n",
            "487 [D loss : 0.848373, acc : 46.29%] [G loss : 0.491991]\n",
            "488 [D loss : 0.820355, acc : 49.80%] [G loss : 0.454589]\n",
            "489 [D loss : 0.840947, acc : 48.05%] [G loss : 0.462512]\n",
            "490 [D loss : 0.828489, acc : 48.05%] [G loss : 0.481052]\n",
            "491 [D loss : 0.849190, acc : 46.09%] [G loss : 0.468237]\n",
            "492 [D loss : 0.825657, acc : 50.20%] [G loss : 0.470432]\n",
            "493 [D loss : 0.852451, acc : 47.46%] [G loss : 0.474349]\n",
            "494 [D loss : 0.849757, acc : 48.44%] [G loss : 0.472702]\n",
            "495 [D loss : 0.838521, acc : 48.63%] [G loss : 0.474931]\n",
            "496 [D loss : 0.835715, acc : 49.61%] [G loss : 0.476605]\n",
            "497 [D loss : 0.858064, acc : 49.41%] [G loss : 0.479818]\n",
            "498 [D loss : 0.859437, acc : 46.68%] [G loss : 0.479776]\n",
            "499 [D loss : 0.852140, acc : 47.07%] [G loss : 0.462740]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5tB0RNIob3v"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}